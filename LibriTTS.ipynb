{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b71fdd0b-1e43-4120-994c-c43a89aebc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dad20a-fdb6-4fa6-99d4-92430d3d3b73",
   "metadata": {},
   "source": [
    "Для обучения возьмем 2 датасета: dev-clean.tar.gz и dev-other.tar.gz (more challenging), объединив их в один\n",
    "Для тестирования соответственно: test-clean.tar.gz и test-other.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f22119b-714a-429c-b74e-1a0b5e87ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = '/Users/a.gorlenko/made/speech/train_data'\n",
    "TEST_DATA_DIR = '/Users/a.gorlenko/made/speech/test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aefc55c2-98f2-41c8-868d-3565b9ff64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MFCC = 13\n",
    "N_FFT_COEF = 0.025\n",
    "HOP_LENGTH_COEF = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc98c409-a79d-41ac-afd3-b82fceb0d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender(reader_df, reader_id):\n",
    "    return reader_df.loc[reader_id]['GENDER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d5066099-6ac6-4220-8b44-c97de2b98504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_audio_files(data_dir):\n",
    "    audio_files = []\n",
    "    for file_dir, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                audio_files.append((file_dir, file))\n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15321a-2736-4dd7-9157-46841494d332",
   "metadata": {},
   "source": [
    "Преобразуем аудиофайлы (wav) из указанной директории (и ее поддиректорий) в данные для обучения.\n",
    "В качестве фичей возьмем:\n",
    "- Мел-кепстральные коэффициенты mfcc\n",
    "- данные по высоте звука (мужской голос, как правило, ниже)\n",
    "\n",
    "Изначально получаем для каждого файла набор векторов, но хочется работать с одним векторов для файла. Поэтому сагрегируем данные для каждого файла. Для mfcc возьмем:\n",
    "- среднее\n",
    "- дисперсию\n",
    "- минимум\n",
    "- максимум\n",
    "\n",
    "Как показали опыты, каждый из этих показателей дает прибавку в качестве.\n",
    "\n",
    "Для высоты возьмем просто среднее (другое не дало особой прибавки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9bf1d574-30c7-410e-9ce9-086830b362c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(data_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    reader_df = pd.read_csv(os.path.join(data_dir, 'LibriTTS', 'speakers.tsv'), \n",
    "                            sep='\\t', \n",
    "                            names=['READER', 'GENDER', 'SUBSET', 'NAME'], \n",
    "                            header=0, \n",
    "                            index_col=0)\n",
    "    \n",
    "    audio_files = find_audio_files(data_dir)\n",
    "    for dir_path, file_name in tqdm(audio_files):\n",
    "        reader_id = int(file_name.split('_')[0])\n",
    "        audio_data, sr = librosa.load(os.path.join(dir_path, file_name), sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=audio_data, \n",
    "                                    sr=sr, \n",
    "                                    n_mfcc=N_MFCC, \n",
    "                                    n_fft=int(sr * N_FFT_COEF), \n",
    "                                    hop_length=int(sr * HOP_LENGTH_COEF))\n",
    "        pitches, magnitudes = librosa.piptrack(y=audio_data, \n",
    "                                               sr=sr, \n",
    "                                               n_fft=int(sr * N_FFT_COEF), \n",
    "                                               hop_length=int(sr * HOP_LENGTH_COEF))\n",
    "        X.append(np.concatenate((\n",
    "            np.mean(mfcc, axis=1), \n",
    "            np.std(mfcc, axis=1), \n",
    "            np.max(mfcc, axis=1), \n",
    "            np.min(mfcc, axis=1),\n",
    "            np.mean(pitches, axis=1),\n",
    "        )))\n",
    "        y.append(get_gender(reader_df, reader_id))\n",
    "    return np.array(X), np.array(y) == 'F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21faffbc-5093-421b-a88e-7d9d18d03f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc52606edd6426aaf98f171c1e8ec94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = build_df(TRAIN_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9060f9-c6c2-4622-9b37-b539a0a18e3f",
   "metadata": {},
   "source": [
    "Убедимся, что имеем дело со сбалансированными классами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f8e691b-6415-443f-8e7e-86c8da8025b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([4932, 5417]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12aca80-a69b-4d8f-b932-fdbf4b1fae68",
   "metadata": {},
   "source": [
    "Попробуем проверить на кроссвалидации несколько моделей классификаторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ba1e311-2a9b-4354-a6df-0027a9d2b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "svc = SVC(probability=True, random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ceb8134-1b54-4fc0-83e7-85d735162c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('random forest', rfc),\n",
    "    ('svc', svc),\n",
    "    ('xgboost', xgb)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e436948-0e75-41dc-891f-12f426934cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest: 0.9254543458795297\n",
      "svc: 0.9297284380775866\n",
      "xgboost: 0.9467499390184342\n"
     ]
    }
   ],
   "source": [
    "for classifier_name, classifier in classifiers:\n",
    "    scores = cross_val_score(classifier, X, y, scoring='roc_auc', cv=3)\n",
    "    print(f'{classifier_name}: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b284163-1a41-4ff2-91fc-f0fad696b2fc",
   "metadata": {},
   "source": [
    "В целом даже без особого подбора гиперпараметров все классификаторы показали неплохие результаты.\n",
    "\n",
    "Посмотрим результаты xgboost на тесте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6234cff7-8f14-4af8-a4b3-8bc32e9b65b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd877c8e-4688-4818-8a1f-880382f39e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5cb1cc4a8c44abbb66c50f8de6bc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9957 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test, y_test = build_df(TEST_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39e5cb19-919c-4d0b-9e96-21344d95a522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9520462549580319"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b540107c-2826-4546-8cc2-b1c6aba22051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.893140504167922"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8f30f1d-8783-4e1a-ae77-4a4cf2882f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9028487947406866"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65810c-72d7-4a8b-bd57-0b7cb33af19b",
   "metadata": {},
   "source": [
    "Видим, что использование mfcc и высоты голоса в качестве фичей позволяют добиться достаточно неплохого качества в классификации мужского/женского голоса даже без тщательного подбора геперпараметров и использования сложных моделей (например, нейросетей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6d320-27db-4602-aabe-fd25b00fae83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
